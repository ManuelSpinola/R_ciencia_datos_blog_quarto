---
title: "Machine Learning (Aprendizaje Autom√°tico) supervisado"

subtitle: "Gu√≠a pr√°ctica de Machine Learning supervisado"

image: "imagenes/tree.png"

author:
  - name: "Manuel Sp√≠nola"
    url: http://www.icomvis.una.ac.cr/index.php/manuel
    affiliation: ICOMVIS - UNA
    affiliation-url: http://www.icomvis.una.ac.cr/
    orcid: 0000-0002-7839-1908

format:
  html:
    embed-resources: true

date: "2025-08-11"

citation: true

toc: true
toc-title: Tabla de contenido
toc-location: right
toc-depth: 4

lang: es

editor: source

categories:
  - "Ciencia de datos"
  - "Machine learning"
---

```{r}
#| messaqge: false
#| warning: false
#| include: false
library(tidyverse)
library(easystats)
library(gt)
library(fastml)
```

# üìå ¬øQu√© es el machine learning (aprendizaje autom√°tico)?

El machine learning (aprendizaje autom√°tico) es una rama de la inteligencia artificial que permite a las computadoras aprender patrones a partir de datos, sin ser programadas expl√≠citamente. Se usa para hacer predicciones o tomar decisiones basadas en nuevos datos.

# üìå Tipos de aprendizaje y modelos

Existen principalmente tres tipos:

-   Aprendizaje supervisado: el modelo aprende a partir de datos etiquetados (con una variable respuesta conocida). Ej: predicci√≥n de precios.

-   Aprendizaje no supervisado: el modelo encuentra patrones o estructuras en datos sin etiquetas. Ej: segmentaci√≥n de clientes.

-   Aprendizaje por refuerzo: el modelo aprende a trav√©s de ensayo y error, optimizando una recompensa.

Los modelos son algoritmos que implementan estos tipos de aprendizaje (como √°rboles de decisi√≥n, redes neuronales, etc.).

## Tabla comparativa de tipos de aprendizaje

```{r}
#| echo: false
library(gt)
library(dplyr)

# Crear el data frame
tabla <- tibble(
  tipo = c("Supervisado", "No supervisado", "Por refuerzo"),
  etiquetas = c("S√≠", "No", "No directamente"),
  objetivo = c(
    "Predecir una salida (Y)",
    "Encontrar estructuras (clusters, etc.)",
    "Maximizar recompensa a largo plazo"
  )
)

# Crear la tabla gt con encabezado coloreado y striping personalizado
tabla |>
  gt() |>
  cols_label(
    tipo = "Tipo de aprendizaje",
    etiquetas = "¬øTiene etiquetas?",
    objetivo = "¬øObjetivo?"
  ) |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  ) |> 
  tab_options(table.width = pct(100))
```


## Resumen: Tipos de Aprendizaje Autom√°tico

```{r}
#| echo: false
library(gt)

# Crear los datos
ml_table <- tibble::tibble(
  `Tipo de aprendizaje` = c("Aprendizaje supervisado", "Aprendizaje no supervisado", "Aprendizaje por refuerzo"),
  `Descripci√≥n` = c(
    "El modelo aprende a partir de datos etiquetados, es decir, cada entrada tiene una salida conocida.",
    "El modelo encuentra patrones o estructuras ocultas en datos sin etiquetas.",
    "El modelo aprende a trav√©s de ensayo y error, optimizando una se√±al de recompensa."
  ),
  `Ejemplos` = c(
    "Regresi√≥n lineal, random forest, redes neuronales para clasificaci√≥n.",
    "An√°lisis de conglomerados (clustering), reducci√≥n de dimensionalidad (PCA).",
    "Agentes que aprenden a jugar videojuegos o a controlar robots mediante retroalimentaci√≥n."
  )
)

# Crear tabla con gt
gt(ml_table) |>
  cols_label(
    `Tipo de aprendizaje` = "Tipo",
    `Descripci√≥n` = "¬øC√≥mo aprende el modelo?",
    `Ejemplos` = "Ejemplos comunes"
  ) |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  )
```




# üìå Clasificaci√≥n vs Regresi√≥n

Son los dos tipos principales de problemas en aprendizaje supervisado:

- **Clasificaci√≥n:** predice categor√≠as o clases. Ej: ‚Äúpresente‚Äù o ‚Äúausente‚Äù.

- **Regresi√≥n:** predice un valor num√©rico continuo. Ej: precio de una casa.

# üìå Flujo de trabajo t√≠pico en machine learning

Pasos que normalmente se siguen al construir un modelo:

1.  Cargar y explorar los datos.
2.  Preprocesar (limpiar y transformar) los datos.
3.  Dividir los datos en entrenamiento y prueba.
4.  Entrenar el modelo.
5.  Evaluar el modelo.
6.  Ajustar hiperpar√°metros si es necesario.
7.  Usar el modelo para predecir nuevos datos.

# üìå Diferencias entre Modelos Lineales y Algoritmos de Machine Learning

Tanto los modelos lineales (como regresi√≥n lineal o log√≠stica) como algoritmos como Random Forest forman parte del mismo ecosistema, pero efectivamente tienen or√≠genes conceptuales distintos, y por eso muchas veces se hace una distinci√≥n sutil pero √∫til entre ellos.

### 1. Modelos estad√≠sticos cl√°sicos

- Ejemplos: regresi√≥n lineal, regresi√≥n log√≠stica, modelos lineales generalizados (GLM), GAM, etc.

- Origen: estad√≠stica tradicional.

- Caracter√≠sticas:

  - Interpretables.
  - Basados en suposiciones expl√≠citas (linealidad, distribuci√≥n de errores, etc.).
  - Tienen una forma funcional definida, por ejemplo:

$$
Y = \beta_0 + \beta_1 X_1 + \dots + \epsilon).
$$

üëâ En ML se los suele llamar:

-   Modelos lineales

-   Modelos param√©tricos

-   O simplemente ‚Äúmodelos estad√≠sticos‚Äù

---

### 2. Algoritmos de aprendizaje autom√°tico (ML algorithms)

-   Ejemplos: Random Forest, Gradient Boosting, Support Vector Machines, redes neuronales, k-NN, etc.

- Origen: inteligencia artificial, ciencias de la computaci√≥n.

- Caracter√≠sticas:

  - Menos supuestos estad√≠sticos.
  - Pueden modelar relaciones complejas y no lineales.
  - A menudo considerados modelos de caja negra (menos interpretables).
  - Suelen requerir m√°s datos y computaci√≥n.

üëâ En ML se los suele llamar:

- Algoritmos de machine learning 

- Modelos de aprendizaje no param√©trico 

- Modelos complejos o no lineales

---

## Comparaci√≥n entre modelos estad√≠sticos cl√°sicos y algoritmos de Machine Learning

```{r}
#| echo: false
library(gt)
library(tibble)

# Crear tabla de comparaci√≥n
tabla <- tibble(
  `Categor√≠a` = c("Modelos estad√≠sticos cl√°sicos", "Algoritmos de Machine Learning"),
  `Ejemplos` = c("Regresi√≥n lineal, regresi√≥n log√≠stica, GLM, GAM",
                 "Random Forest, SVM, XGBoost, Redes neuronales, k-NN"),
  `Origen` = c("Estad√≠stica tradicional", "Ciencias de la computaci√≥n / IA"),
  `Caracter√≠sticas` = c("Interpretables, basados en supuestos como linealidad y normalidad, f√≥rmula funcional definida",
                        "Menos supuestos, pueden modelar relaciones no lineales, suelen ser cajas negras"),
  `T√©rminos sugeridos` = c("Modelos lineales, modelos estad√≠sticos, modelos param√©tricos",
                           "Algoritmos de ML, modelos complejos, modelos no param√©tricos")
)

# Generar tabla con gt
tabla_gt <- tabla |>
  gt() |>
  cols_label(
    `Categor√≠a` = "Categor√≠a",
    `Ejemplos` = "Ejemplos",
    `Origen` = "Origen",
    `Caracter√≠sticas` = "Caracter√≠sticas",
    `T√©rminos sugeridos` = "T√©rminos sugeridos"
  ) |>
  cols_align(align = "left", columns = everything()) |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  )

# Mostrar la tabla
tabla_gt
```

üìò **Ejemplo en una frase:**

‚ÄúSe compararon modelos estad√≠sticos cl√°sicos (regresi√≥n log√≠stica) con algoritmos de machine learning (Random Forest, XGBoost) para predecir la presencia de la especie.‚Äù

# üìå Hiperpar√°metros en machine learning

Los hiperpar√°metros son configuraciones externas al modelo que se establecen antes del entrenamiento y afectan su rendimiento. No se aprenden directamente de los datos, a diferencia de los par√°metros del modelo (como coeficientes en regresi√≥n). 

## Ejemplos comunes de hiperpar√°metros para algoritmos de regresi√≥n

```{r}
#| include: false
#| label: tbl-fastml-regresion-ejemplos
#| tbl-cap: Hiperpar√°metros por algoritmo de regresi√≥n con ejemplos
#| echo: false
library(gt)
library(tibble)

tabla <- tribble(
  ~Algoritmo, ~Hiperparametros, ~Descripci√≥n, ~Ejemplos,

  "Regresi√≥n lineal", 
  "Intercepto, normalizaci√≥n", 
  "Ajustar o no el intercepto; normalizar predictores para mejorar estabilidad.", 
  "fit_intercept = TRUE, normalize = FALSE",

  "Ridge", 
  "Penalizaci√≥n (alpha)", 
  "Evita sobreajuste reduciendo la magnitud de los coeficientes (L2).", 
  "alpha = 1.0",

  "Lasso", 
  "Penalizaci√≥n (alpha), n√∫mero m√°ximo de iteraciones", 
  "Puede reducir coeficientes a cero (L1), √∫til para selecci√≥n de variables.", 
  "alpha = 0.1, max_iter = 1000",

  "Elastic Net", 
  "Penalizaci√≥n (alpha), proporci√≥n L1 (l1_ratio), max_iter", 
  "Combina Lasso y Ridge; √∫til cuando hay muchas variables correlacionadas.", 
  "alpha = 0.1, l1_ratio = 0.5",

  "√Årbol de decisi√≥n", 
  "Profundidad m√°xima, min muestras por divisi√≥n y hoja, max variables", 
  "Controla la complejidad del √°rbol; puede sobreajustar si no se regula.", 
  "max_depth = 5, min_samples_split = 10",

  "Random Forest", 
  "N√∫mero de √°rboles, profundidad m√°xima, muestras m√≠nimas, variables", 
  "Promedio de predicciones de varios √°rboles; mejora estabilidad.", 
  "n_estimators = 100, max_features = 'sqrt'",

  "XGBoost", 
  "N√∫mero de √°rboles, tasa de aprendizaje, profundidad, subsample", 
  "Boosting que mejora iterativamente; muy eficaz para datos estructurados.", 
  "nrounds = 100, eta = 0.1, max_depth = 6",

  "LightGBM", 
  "Num hojas, tasa de aprendizaje, iteraciones, subsample", 
  "Algoritmo de boosting r√°pido y eficiente; ideal para datasets grandes.", 
  "num_leaves = 31, learning_rate = 0.05",

  "SVM lineal", 
  "C (penalizaci√≥n), tolerancia (epsilon)", 
  "Modelo de regresi√≥n lineal con margen de error flexible.", 
  "C = 1.0, epsilon = 0.1",

  "SVM RBF", 
  "C (penalizaci√≥n), gamma, tolerancia (epsilon)", 
  "Kernel RBF permite modelar relaciones no lineales.", 
  "C = 1.0, gamma = 'scale'",

  "Vecinos m√°s cercanos", 
  "N√∫mero de vecinos (k), m√©trica de distancia, ponderaci√≥n", 
  "Predice a partir del promedio de vecinos m√°s cercanos.", 
  "n_neighbors = 5, weights = 'uniform'",

  "Perceptr√≥n multicapa (MLP)", 
  "Capas ocultas, activaci√≥n, tasa de aprendizaje, penalizaci√≥n", 
  "Red neuronal para regresi√≥n; requiere ajuste fino de arquitectura.", 
  "hidden = c(100, 50), activation = 'relu', alpha = 0.0001",

  "PLS", 
  "N√∫mero de componentes, escalado", 
  "Reduce dimensionalidad conservando relaci√≥n con variable respuesta.", 
  "n_components = 5, scale = TRUE",

  "Regresi√≥n bayesiana", 
  "Alpha, lambda (par√°metros del prior)", 
  "Versi√≥n bayesiana de Ridge; introduce incertidumbre en los coeficientes.", 
  "alpha = 1.0, lambda = 0.1"
)

# Crear tabla gt
tabla %>%
  gt() %>%
  tab_header(
    title = "Hiperpar√°metros por algoritmo de regresi√≥n (conceptuales + ejemplos)"
  ) %>%
  cols_label(
    Algoritmo = "Algoritmo",
    Hiperparametros = "Hiperpar√°metros comunes",
    Descripci√≥n = "Descripci√≥n",
    Ejemplos = "Ejemplos"
  ) %>%
  cols_width(
    everything() ~ px(250)
  ) |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  )
```


```{r}
#| echo: false
##| label: tbl-fastml-regresion-ejemplos-agnosticos
##| tbl-cap: Hiperpar√°metros por algoritmo de regresi√≥n con ejemplos agn√≥sticos

library(gt)
library(tibble)

tabla <- tribble(
  ~Algoritmo, ~Hiperparametros, ~Descripci√≥n, ~Ejemplos,

  "Regresi√≥n lineal", 
  "Intercepto, normalizaci√≥n", 
  "Ajustar o no el intercepto; normalizar predictores para mejorar estabilidad.", 
  "Con/sin intercepto; con/sin normalizaci√≥n",

  "Ridge", 
  "Penalizaci√≥n (L2)", 
  "Evita sobreajuste reduciendo la magnitud de los coeficientes.", 
  "Penalizaci√≥n moderada (e.g., 1.0)",

  "Lasso", 
  "Penalizaci√≥n (L1), n√∫mero m√°ximo de iteraciones", 
  "Puede reducir coeficientes a cero; √∫til para selecci√≥n de variables.", 
  "Penalizaci√≥n baja (e.g., 0.1); 1000 iteraciones",

  "Elastic Net", 
  "Proporci√≥n L1 vs L2, penalizaci√≥n total, iteraciones", 
  "Combina Lasso y Ridge; √∫til con muchas variables correlacionadas.", 
  "Proporci√≥n 50/50; penalizaci√≥n 0.1",

  "√Årbol de decisi√≥n", 
  "Profundidad m√°xima, min. muestras por divisi√≥n y hoja, variables por nodo", 
  "Controla la complejidad del √°rbol para evitar sobreajuste.", 
  "Profundidad = 5; m√≠nimo 10 muestras por divisi√≥n",

  "Random Forest", 
  "N√∫mero de √°rboles, profundidad, muestras m√≠nimas, variables por divisi√≥n", 
  "Promedio de varios √°rboles para mejorar estabilidad.", 
  "100 √°rboles; ra√≠z cuadrada del total de variables por divisi√≥n",

  "XGBoost", 
  "N√∫mero de √°rboles, tasa de aprendizaje, profundidad, fracci√≥n de datos", 
  "Boosting que mejora iterativamente; eficaz en datos estructurados.", 
  "100 iteraciones; tasa de aprendizaje baja (e.g., 0.1)",

  "LightGBM", 
  "N√∫mero de hojas, tasa de aprendizaje, iteraciones, fracci√≥n de datos", 
  "Boosting r√°pido y eficiente; ideal para conjuntos grandes.", 
  "32 hojas; tasa de aprendizaje media (e.g., 0.05)",

  "SVM lineal", 
  "Penalizaci√≥n (C), margen de tolerancia (epsilon)", 
  "Modelo lineal con margen de error flexible.", 
  "Penalizaci√≥n media (e.g., 1.0); tolerancia baja",

  "SVM RBF", 
  "Penalizaci√≥n (C), ancho del kernel (gamma), tolerancia", 
  "Kernel RBF permite capturar relaciones no lineales.", 
  "Penalizaci√≥n 1.0; gamma peque√±o (e.g., 0.01)",

  "Vecinos m√°s cercanos", 
  "N√∫mero de vecinos (k), m√©trica de distancia, ponderaci√≥n", 
  "Promedia los valores de los vecinos m√°s cercanos.", 
  "5 vecinos; distancia euclidiana; ponderaci√≥n uniforme",

  "Perceptr√≥n multicapa (MLP)", 
  "Capas ocultas, funci√≥n de activaci√≥n, tasa de aprendizaje, regularizaci√≥n", 
  "Red neuronal; requiere ajuste fino de arquitectura.", 
  "2 capas; 100 y 50 neuronas; tasa de aprendizaje baja",

  "PLS", 
  "N√∫mero de componentes, escalado", 
  "Extrae componentes que maximizan relaci√≥n entre predictores y respuesta.", 
  "5 componentes; predictores escalados",

  "Regresi√≥n bayesiana", 
  "Par√°metros del prior (alpha, lambda)", 
  "Introduce incertidumbre en los coeficientes con enfoque bayesiano.", 
  "Prior informativo moderado"
)

# Crear tabla gt
tabla %>%
  gt() %>%
  tab_header(
    title = "Hiperpar√°metros por algoritmo de regresi√≥n",
    subtitle = "Descripci√≥n y ejemplos agn√≥sticos (sin referencia a sintaxis espec√≠fica)"
  ) %>%
  cols_label(
    Algoritmo = "Algoritmo",
    Hiperparametros = "Hiperpar√°metros comunes",
    Descripci√≥n = "Descripci√≥n",
    Ejemplos = "Ejemplos conceptuales"
  ) %>%
  cols_width(
    everything() ~ px(250)
  ) |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  )
```


## Ejemplos comunes de hiperpar√°metros para algoritmos de clasificaci√≥n

```{r}
#| include: false
#| label: tbl-fastml-clasificacion-ejemplos
#| tbl-cap: Hiperpar√°metros por algoritmo de clasificaci√≥n con ejemplos
#| echo: false
library(gt)
library(tibble)

tabla <- tribble(
  ~Algoritmo, ~Hiperparametros, ~Descripci√≥n, ~Ejemplos,

  "Regresi√≥n log√≠stica", 
  "Penalizaci√≥n (L1 o L2), regularizaci√≥n (C)", 
  "Controla el sobreajuste penalizando los coeficientes.", 
  "penalty = 'l2', C = 1.0",

  "Regresi√≥n multinomial", 
  "Penalizaci√≥n, m√©todo de optimizaci√≥n", 
  "Clasificaci√≥n multiclase basada en regresi√≥n log√≠stica.", 
  "penalty = 'l1', solver = 'saga'",

  "√Årbol de decisi√≥n", 
  "Profundidad m√°xima, min muestras para dividir, min muestras en hoja", 
  "Limita la complejidad del √°rbol.", 
  "max_depth = 5, min_samples_split = 10",

  "C5.0 (reglas)", 
  "N√∫mero de reglas, pruning, uso de boosting", 
  "Clasificaci√≥n basada en reglas interpretables.", 
  "trials = 10 (boosting), winnow = TRUE",

  "Random Forest", 
  "N√∫mero de √°rboles, profundidad m√°xima, variables por divisi√≥n", 
  "Modelo de conjunto con √°rboles aleatorios.", 
  "n_estimators = 100, max_features = 'sqrt'",

  "XGBoost", 
  "N√∫mero de √°rboles, tasa de aprendizaje, profundidad, subsample", 
  "Boosting con regularizaci√≥n avanzada.", 
  "nrounds = 100, eta = 0.1, max_depth = 6",

  "LightGBM", 
  "Num hojas, tasa de aprendizaje, iteraciones, subsample", 
  "Boosting r√°pido y eficiente.", 
  "num_leaves = 31, learning_rate = 0.05",

  "SVM lineal", 
  "C (penalizaci√≥n), tolerancia (epsilon)", 
  "Separaci√≥n lineal de clases con margen √≥ptimo.", 
  "C = 1.0, epsilon = 0.1",

  "SVM RBF", 
  "C (penalizaci√≥n), gamma, epsilon", 
  "Separaci√≥n no lineal usando kernel radial.", 
  "C = 1.0, gamma = 'scale'",

  "Vecinos m√°s cercanos", 
  "N√∫mero de vecinos (k), m√©trica de distancia", 
  "Clasificaci√≥n basada en vecinos m√°s cercanos.", 
  "k = 5, metric = 'euclidean'",

  "Naive Bayes", 
  "Tipo de distribuci√≥n, suavizado", 
  "Clasificador probabil√≠stico basado en independencia.", 
  "laplace = 1 (suavizado)",

  "Perceptr√≥n multicapa (MLP)", 
  "Capas ocultas, activaci√≥n, tasa de aprendizaje", 
  "Red neuronal multicapa con retropropagaci√≥n.", 
  "hidden = c(100, 50), activation = 'relu'",

  "Discriminante lineal", 
  "Priorizaci√≥n de clases, escalado", 
  "Modelo lineal para separaci√≥n de clases.", 
  "prior = c(0.5, 0.5)",

  "Discriminante cuadr√°tico", 
  "Escalado, covarianzas por clase", 
  "Permite fronteras curvas entre clases.", 
  "covariance = 'unpooled'",

  "√Årbol por bolsas (Bagging)", 
  "N√∫mero de √°rboles, muestras por bootstrap", 
  "Promedio de m√∫ltiples √°rboles para reducir varianza.", 
  "trees = 50, bootstrap = TRUE"
)

# Crear tabla gt
tabla %>%
  gt() %>%
  tab_header(
    title = "Hiperpar√°metros por algoritmo de clasificaci√≥n (conceptuales + ejemplos)"
  ) %>%
  cols_label(
    Algoritmo = "Algoritmo",
    Hiperparametros = "Hiperpar√°metros comunes",
    Descripci√≥n = "Descripci√≥n",
    Ejemplos = "Ejemplos"
  ) %>%
  cols_width(
    everything() ~ px(250)
  ) |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  )
```

```{r}
#| echo: false
##| label: tbl-fastml-clasificacion-ejemplos-agnosticos
##| tbl-cap: Hiperpar√°metros por algoritmo de clasificaci√≥n con ejemplos agn√≥sticos

library(gt)
library(tibble)

tabla <- tribble(
  ~Algoritmo, ~Hiperparametros, ~Descripci√≥n, ~Ejemplos,

  "Regresi√≥n log√≠stica", 
  "Tipo de penalizaci√≥n (L1 o L2), fuerza de regularizaci√≥n", 
  "Controla el sobreajuste penalizando los coeficientes.", 
  "Penalizaci√≥n L2, regularizaci√≥n media",

  "Regresi√≥n multinomial", 
  "Tipo de penalizaci√≥n, m√©todo de optimizaci√≥n", 
  "Clasificaci√≥n multiclase basada en regresi√≥n log√≠stica.", 
  "Penalizaci√≥n L1, optimizaci√≥n r√°pida",

  "√Årbol de decisi√≥n", 
  "Profundidad m√°xima, m√≠nimo de muestras por divisi√≥n y por hoja", 
  "Limita la complejidad del √°rbol para evitar sobreajuste.", 
  "Profundidad = 5, m√≠nimo 10 muestras por divisi√≥n",

  "C5.0 (reglas)", 
  "Cantidad de reglas, uso de poda, aplicaci√≥n de boosting", 
  "Clasificaci√≥n basada en reglas interpretables.", 
  "10 iteraciones con boosting, poda activada",

  "Random Forest", 
  "N√∫mero de √°rboles, profundidad, n√∫mero de variables por divisi√≥n", 
  "Modelo de conjunto que combina muchos √°rboles.", 
  "100 √°rboles, variables seleccionadas aleatoriamente por divisi√≥n",

  "XGBoost", 
  "N√∫mero de iteraciones, tasa de aprendizaje, profundidad, fracci√≥n de muestra", 
  "Boosting con regularizaci√≥n avanzada para clasificaci√≥n eficaz.", 
  "100 iteraciones, tasa de aprendizaje = 0.1, profundidad = 6",

  "LightGBM", 
  "N√∫mero de hojas, tasa de aprendizaje, iteraciones, fracci√≥n de muestra", 
  "Boosting eficiente especialmente para grandes conjuntos.", 
  "32 hojas, tasa de aprendizaje media",

  "SVM lineal", 
  "Penalizaci√≥n (C), margen de tolerancia", 
  "Separaci√≥n lineal con margen √≥ptimo.", 
  "Penalizaci√≥n media, tolerancia baja",

  "SVM RBF", 
  "Penalizaci√≥n (C), gamma del kernel, tolerancia", 
  "Kernel RBF para relaciones no lineales.", 
  "Gamma peque√±o, penalizaci√≥n media",

  "Vecinos m√°s cercanos", 
  "N√∫mero de vecinos (k), m√©trica de distancia", 
  "Clasificaci√≥n en base a vecinos cercanos en el espacio.", 
  "5 vecinos, distancia euclidiana",

  "Naive Bayes", 
  "Distribuci√≥n asumida, suavizado", 
  "Clasificador probabil√≠stico basado en independencia de predictores.", 
  "Distribuci√≥n gaussiana, suavizado aplicado",

  "Perceptr√≥n multicapa (MLP)", 
  "Capas ocultas, funci√≥n de activaci√≥n, tasa de aprendizaje", 
  "Red neuronal multicapa entrenada por retropropagaci√≥n.", 
  "2 capas ocultas, activaci√≥n ReLU, tasa de aprendizaje baja",

  "Discriminante lineal", 
  "Priorizaci√≥n de clases, escalado de variables", 
  "Modelo lineal para separar clases con varianzas iguales.", 
  "Clases balanceadas, predictores escalados",

  "Discriminante cuadr√°tico", 
  "Escalado de variables, covarianza espec√≠fica por clase", 
  "Permite separaci√≥n no lineal entre clases.", 
  "Varianzas distintas por clase, escalado activado",

  "√Årbol por bolsas (Bagging)", 
  "N√∫mero de √°rboles, tama√±o de muestra bootstrap", 
  "Promedia m√∫ltiples √°rboles sobre subconjuntos re-muestreados.", 
  "50 √°rboles, muestras re-muestreadas con reemplazo"
)

# Crear tabla gt
tabla %>%
  gt() %>%
  tab_header(
    title = "Hiperpar√°metros por algoritmo de clasificaci√≥n",
    subtitle = "Descripci√≥n y ejemplos agn√≥sticos (sin referencia a sintaxis espec√≠fica)"
  ) %>%
  cols_label(
    Algoritmo = "Algoritmo",
    Hiperparametros = "Hiperpar√°metros comunes",
    Descripci√≥n = "Descripci√≥n",
    Ejemplos = "Ejemplos conceptuales"
  ) %>%
  cols_width(
    everything() ~ px(250)
  ) |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  )
```


# üìå Evaluaci√≥n y Validaci√≥n en Machine Learning

En el contexto de Machine Learning, validaci√≥n y evaluaci√≥n son dos conceptos fundamentales pero distintos en el proceso de construir y analizar modelos predictivos. Aqu√≠ te explico claramente cada uno:

---

### üîÑ 1. Validaci√≥n

Validaci√≥n es el proceso de ajustar y seleccionar modelos (por ejemplo, elegir hiperpar√°metros) utilizando un conjunto de datos diferente del de entrenamiento y del de prueba, llamado conjunto de validaci√≥n.

**Objetivo:**

- Evitar el sobreajuste (overfitting)

- Encontrar la mejor versi√≥n del modelo antes de evaluarlo definitivamente.

**Tipos comunes de validaci√≥n:**

- Hold-out validation: dividir los datos en entrenamiento / validaci√≥n / prueba.

- Cross-validation (validaci√≥n cruzada):

- Se entrena y valida el modelo varias veces con diferentes particiones de los datos.

- La m√°s com√∫n es la k-fold cross-validation.

---

### ‚úÖ  2. Evaluaci√≥n

Evaluaci√≥n es el proceso de medir el rendimiento de un modelo ya entrenado, generalmente utilizando datos que nunca ha visto durante el entrenamiento, llamados datos de prueba o test set.

**Objetivo:** 

- Saber qu√© tan bien generaliza el modelo a datos nuevos

**Ejemplos de m√©tricas de evaluaci√≥n:**

- Para regresi√≥n:
    - RMSE (Root Mean Squared Error)
    - MAE (Mean Absolute Error)
    - R¬≤ (Coeficiente de determinaci√≥n)

- Para clasificaci√≥n:
    - Exactitud (accuracy)
    - Precisi√≥n, recall, F1-score
    - AUC-ROC
    
---

üß† **Analog√≠a**

Imagina que entren√°s a un estudiante para un examen:

- Entrenamiento: el estudiante estudia con ejercicios.

- Validaci√≥n: le das pruebas tipo ensayo para ver si est√° aprendiendo bien y ajustar su estudio.

- Evaluaci√≥n: es el examen real que toma para obtener su calificaci√≥n.

### Diferencias entre Validaci√≥n y Evaluaci√≥n en Machine Learning

```{r}
#| echo: false
# Crear los datos
tabla <- tibble::tibble(
  Concepto = c("Validaci√≥n", "Evaluaci√≥n"),
  `¬øCu√°ndo se usa?` = c("Durante el entrenamiento", "Despu√©s de entrenar y validar"),
  `¬øCon qu√© datos?` = c("Conjunto de validaci√≥n", "Conjunto de prueba"),
  `¬øPara qu√© sirve?` = c("Ajustar hiperpar√°metros, prevenir sobreajuste", 
                         "Medir rendimiento final del modelo")
)

# Crear la tabla con gt
tabla <- tabla %>%
  gt() |>
  cols_label(
    Concepto = "Concepto",
    `¬øCu√°ndo se usa?` = "¬øCu√°ndo se usa?",
    `¬øCon qu√© datos?` = "¬øCon qu√© datos?",
    `¬øPara qu√© sirve?` = "¬øPara qu√© sirve?"
  ) |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  )

# Mostrar la tabla
tabla
```

# üìå Detecci√≥n y manejo del sobreajuste

Detectar y manejar el sobreajuste (overfitting) es fundamental en machine learning, ya que un modelo sobreajustado aprende demasiado bien los datos de entrenamiento, incluyendo el ruido, y no generaliza bien a datos nuevos.

---

## ¬øC√≥mo se detecta el sobreajuste?

**1. Comparando desempe√±o en entrenamiento vs validaci√≥n/test**

- Eval√∫a el modelo en el conjunto de entrenamiento y en el conjunto de validaci√≥n o test.

- Indicadores de sobreajuste:

- Muy buen desempe√±o en entrenamiento (por ejemplo, accuracy 98%)

- Desempe√±o mucho peor en validaci√≥n/test (por ejemplo, accuracy 72%)

Esto se hace con m√©tricas como:

- Error cuadr√°tico medio (RMSE)
- Precisi√≥n, Recall, F1
- AUC-ROC
- etc., seg√∫n el tipo de problema

**2. Curvas de aprendizaje (learning curves)**

-   Se grafican los errores en entrenamiento y validaci√≥n a medida que se entrena el modelo con m√°s datos.

-   Si el error en validaci√≥n se mantiene alto mientras el de entrenamiento baja, hay sobreajuste.

---

## ¬øC√≥mo se maneja o reduce el sobreajuste?

**1. Cross-validation**

-   T√©cnicas como k-fold cross-validation te dan una evaluaci√≥n m√°s robusta.

-   Ayuda a detectar si el buen desempe√±o es solo suerte en una partici√≥n de datos.

**2. Reducir la complejidad del modelo**

- Usar modelos m√°s simples:
- Menos nodos en un √°rbol
- Menos capas o neuronas en una red
- Menos t√©rminos en un modelo lineal
- O regularizar el modelo (ver m√°s abajo)

**3. Regularizaci√≥n**

- Penaliza la complejidad del modelo:
- L1 (Lasso): fuerza a que coeficientes irrelevantes sean 0
- L2 (Ridge): reduce la magnitud de los coeficientes
- Algunos modelos que incluyen regularizaci√≥n:
- glmnet, xgboost, keras, etc.

**4. M√°s datos**

- Aumentar el tama√±o del conjunto de entrenamiento puede ayudar a que el modelo generalice mejor.

**5. Data augmentation (especialmente en im√°genes, texto)**

- Crear versiones modificadas de los datos para entrenar (rotaci√≥n, ruido, traducci√≥n‚Ä¶)

**6. Early stopping**

- Detener el entrenamiento tan pronto como el error de validaci√≥n comience a aumentar.

**7. Dropout y otras t√©cnicas (redes neuronales)**

- Dropout apaga neuronas aleatoriamente durante el entrenamiento para evitar dependencia excesiva.

### Tabla comparativa de la detecci√≥n y manejo del sobreajuste

```{r}
#| echo: false
library(gt)

tabla <- tibble::tibble(
  Paso = c("Detecci√≥n", "Manejo"),
  T√©cnicas = c(
    "Comparar m√©tricas entre entrenamiento y validaci√≥n; usar curvas de aprendizaje",
    "Cross-validation, regularizaci√≥n (L1, L2), early stopping, modelos m√°s simples"
  )
)

tabla |>
  gt() |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  ) |> 
  tab_options(table.width = pct(100))
```

# üìå Elecci√≥n de m√©tricas seg√∫n el tipo de modelo

La elecci√≥n de m√©tricas de evaluaci√≥n es crucial para medir el rendimiento de un modelo de machine learning. Dependiendo del tipo de problema (regresi√≥n o clasificaci√≥n), algunas m√©tricas son m√°s adecuadas que otras.

### Tabla de m√©tricas comunes para regresi√≥n

```{r}
#| echo: false
#| message: false
#| warning: false

library(gt)
library(dplyr)

tabla <- tibble::tibble(
  `M√©trica` = c(
    "RMSE (Root Mean Squared Error)",
    "MAE (Mean Absolute Error)",
    "R¬≤ (Coeficiente de determinaci√≥n)",
    "MAPE (Mean Absolute Percentage Error)",
    "MSE (Mean Squared Error)",
    "Huber Loss"
  ),
  `Significado` = c(
    "Ra√≠z cuadrada del error cuadr√°tico medio. Penaliza m√°s los errores grandes.",
    "Promedio del valor absoluto de los errores. M√°s robusto a outliers que el RMSE.",
    "Proporci√≥n de la varianza explicada por el modelo. 1 es perfecto, 0 significa que no mejora sobre la media.",
    "Porcentaje promedio del error absoluto. √ötil para interpretar errores en t√©rminos relativos.",
    "Promedio de los errores al cuadrado. Muy sensible a valores at√≠picos.",
    "Combina MAE y MSE, penalizando menos los errores peque√±os y siendo m√°s robusto a outliers."
  )
)

gt(tabla) |>
  cols_label(
    `M√©trica` = "M√©trica",
    `Significado` = "Descripci√≥n"
  ) |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  )
```


### Tabla de m√©tricas comunes para clasificaci√≥n binaria

```{r}
#| echo: false
#| message: false
#| warning: false

library(gt)
library(dplyr)

tabla <- tibble::tibble(
  `M√©trica` = c(
    "Accuracy",
    "Precision",
    "Recall (Sensibilidad)",
    "Especificidad",
    "F1 Score",
    "ROC AUC",
    "Log Loss",
    "MCC (Coeficiente de correlaci√≥n de Matthews)",
    "Cohen's Kappa (Œ∫)"
  ),
  `Significado` = c(
    "Proporci√≥n de predicciones correctas sobre el total de casos.",
    "De los casos predichos como positivos, qu√© proporci√≥n es realmente positiva.",
    "De los casos realmente positivos, qu√© proporci√≥n fue predicha correctamente.",
    "De los casos realmente negativos, qu√© proporci√≥n fue predicha correctamente.",
    "Media arm√≥nica entre precision y recall, √∫til cuando hay clases desbalanceadas.",
    "√Årea bajo la curva ROC. Eval√∫a la capacidad del modelo para distinguir entre clases.",
    "Penaliza las predicciones incorrectas con mayor severidad, √∫til para probabilidades.",
    "Mide la calidad de las predicciones binarias, especialmente √∫til con clases desbalanceadas.",
    "Mide el acuerdo entre predicci√≥n y verdad ajustado por azar; 1 es acuerdo perfecto, 0 es azar."
  )
)

gt(tabla) |>
  cols_label(
    `M√©trica` = "M√©trica",
    `Significado` = "Descripci√≥n"
  ) |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  )
```


### Tabla de elecci√≥n de m√©tricas seg√∫n el tipo de modelo

```{r}
#| echo: false
library(tibble)
library(gt)

tabla <- tibble::tibble(
  `Tipo de problema` = c("Regresi√≥n", "Regresi√≥n", "Regresi√≥n", "Clasificaci√≥n", "Clasificaci√≥n", "Clasificaci√≥n", "Clasificaci√≥n", "Clasificaci√≥n"),
  `Ejemplo` = c(
    "Predecir precio de una casa",
    "Predecir temperatura diaria",
    "Predecir n√∫mero de visitas",
    "Detectar enfermedad (s√≠/no)",
    "Clasificar correos como spam",
    "Predicci√≥n de fraude bancario",
    "Clasificaci√≥n con clases desbalanceadas",
    "Evaluar desempe√±o m√°s all√° del azar"
  ),
  `M√©tricas recomendadas` = c(
    "RMSE, MAE, R¬≤",
    "RMSE, MAE",
    "MAE, R¬≤",
    "Accuracy, Sensibilidad, Especificidad",
    "F1 Score, ROC AUC",
    "ROC AUC, F1 Score",
    "Sensibilidad, F1 Score, ROC AUC",
    "Kappa"
  )
)

tabla |>
  gt() |>
  # Colorear encabezado
  tab_style(
    style = list(
      cell_fill(color = "#004080"),      # azul oscuro
      cell_text(color = "white", weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) |>
  # Colorear filas impares
  tab_style(
    style = cell_fill(color = "#f0f8ff"),
    locations = cells_body(rows = seq(1, nrow(tabla), 2))
  ) |>
  # Colorear filas pares
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(2, nrow(tabla), 2))
  ) |> 
  tab_options(table.width = pct(100))
```


```{r}
#| include: false
#| echo: false

# Algoritmos de regresi√≥n soportados por fastml

library(tibble)
library(gt)

tibble::tibble(
  Algoritmo = c(
    "linear_reg",
    "ridge_regression",
    "lasso_regression",
    "elastic_net",
    "decision_tree",
    "rand_forest",
    "xgboost",
    "lightgbm",
    "svm_linear",
    "svm_rbf",
    "nearest_neighbor",
    "mlp",
    "pls",
    "bayes_glm"
  ),
  Descripci√≥n = c(
    "Regresi√≥n lineal cl√°sica",
    "Regresi√≥n con penalizaci√≥n L2 (Ridge)",
    "Regresi√≥n con penalizaci√≥n L1 (Lasso)",
    "Combinaci√≥n de L1 y L2 (Elastic Net)",
    "√Årbol de decisi√≥n para regresi√≥n",
    "Bosque aleatorio para regresi√≥n",
    "Extreme Gradient Boosting",
    "Light Gradient Boosting Machine",
    "M√°quinas de soporte vectorial (kernel lineal)",
    "SVM con kernel radial (RBF)",
    "Vecinos m√°s cercanos para regresi√≥n",
    "Perceptr√≥n multicapa (red neuronal simple)",
    "Regresi√≥n por componentes principales (PLS)",
    "Regresi√≥n lineal bayesiana"
  )
) %>%
  gt() %>%
  tab_header(title = "Algoritmos de regresi√≥n soportados por fastml")
```



```{r}
#| include: false
#| echo: false

# Algoritmos de clasificaci√≥n soportados por fastml

library(tibble)
library(gt)

tibble::tibble(
  Algoritmo = c(
    "logistic_reg",
    "multinom_reg",
    "decision_tree",
    "C5_rules",
    "rand_forest",
    "xgboost",
    "lightgbm",
    "svm_linear",
    "svm_rbf",
    "nearest_neighbor",
    "naive_Bayes",
    "mlp",
    "discrim_linear",
    "discrim_quad",
    "bag_tree"
  ),
  Descripci√≥n = c(
    "Regresi√≥n log√≠stica binaria",
    "Regresi√≥n log√≠stica multinomial",
    "√Årbol de decisi√≥n para clasificaci√≥n",
    "Reglas de decisi√≥n tipo C5.0",
    "Bosque aleatorio para clasificaci√≥n",
    "Extreme Gradient Boosting",
    "Light Gradient Boosting Machine",
    "M√°quinas de soporte vectorial (kernel lineal)",
    "SVM con kernel radial (RBF)",
    "Vecinos m√°s cercanos (KNN)",
    "Clasificador bayesiano ingenuo",
    "Perceptr√≥n multicapa (red neuronal simple)",
    "An√°lisis discriminante lineal (LDA)",
    "An√°lisis discriminante cuadr√°tico (QDA)",
    "Ensamble de √°rboles (bagging)"
  )
) %>%
  gt() %>%
  tab_header(title = "Algoritmos de clasificaci√≥n soportados por fastml")
```
